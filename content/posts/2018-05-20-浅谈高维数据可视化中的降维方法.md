---
layout: post
title: '浅谈高维数据可视化中的降维方法'
date: 2018-05-20 01:49:00 +0800
category: from_cnblogs
slug: p20180520014900
---
# 浅谈高维数据可视化中的降维方法

来源：[link]: https://blog.csdn.net/u011001084/article/details/51396447

我们生活在三维空间中，很难直接理解三维以上的空间（爱因斯坦等牛人除外）。但是爱因斯坦这样的人毕竟在人群中是少数，对大多数人来说，高维数据如何进行可视化呢？

聪明的人可以用其他的视觉通道对一些维度进行视觉编码，比如颜色、形状、朝向、体积、半径、表面覆盖物等等。。。不过有两个很显然的问题，1）用户理解起来，不那么方便了。可能要想半天才能反应过来，因为要对这些视觉通道进行一一的反编码，记住这些本身就挺难的。2）维度比较大，不多说，就十几维，恐怕视觉通道已经捉襟见肘，不够用了。

那咋办呢？目前，对高维数据进行可视化主要有三大类方法。

1. 得益于机器学习的发展，降维的方法越来越多。把维度降到2或者3，就可以用非常传统的散点图来将结果进行可视化了。但是降维带来的问题也是显然的，降维的初衷是将原始维度中冗余无用的信息滤掉，不过这个过程很可能不能避免的将有用的信息也丢失掉了。
2. 为了避免上面的问题，第二类方法决定不降维了，直接用散点矩阵把维度直接的两两关系全部展现出来，，这样用户就可以很清楚的看到维度之间的两两关系了嘛。但是你很快就头疼了，10维的散点矩阵，足足有100个小的散点图，这可咋看呀！
3. 第三种方法呢，是想不降维，但同时也不希望出现太多的图让用户去看，最好就一张图。典型的代表方法有平行坐标轴、RadViz、Star coordinates等，以及最近华人学者曹楠老师提出来的UnTangle Map【1】，感兴趣的可以看后面列出来的参考文献。
这篇博客主要介绍第一类方法，降维。线性和非线性各选两三种有代表性的方法讲述。降维方法的分类见下图：


## 线性降维方法
### PCA
可能大部分人接触的第一个降维方法都是PCA，PCA是一种线性的降维方法，举一个二维特征的例子，见下图。这两个特征之间存在着很明显的线性关系，检测出这些线性关系，并且去除它是PCA的目标。具体到下面的这个图，如果我们把x1,x2坐标系换成u1，u2 ，可以看到u1反应了特征的主要变化，而u2的变化比较小，几乎可以忽略不计，那么就可以用u1一个维度代表x1，x2 两个维度，也就达到了降维的目的。


我承认，目前比较火的图像、视频、文字等多媒体数据大多是在一个非线性的流形上或者附近，PCA作为一种线性的降维方法可能不太使用了。但是PCA依然是不可替代的，比如去年IEEE Vis年会中的可视分析（VAST2015）最佳论文就颁给了一个使用PCA来做动态网络分析的工作【2】。


### LDA
与PCA保持数据信息不同，LDA是为了使得降维后的数据点尽可能地容易被区分。既然是线性的，那么就是希望找到映射向量a， 使得 a‘X后的数据点能够保持以下两种性质：

同类数据点尽可能接近
不同类数据点尽可能分开
为了显示与PCA的区别，在网上找来了下面的图，PCA找到的是轴2，LDA找到的是轴1. LDA与PCA的目标不同，LDA希望降维后数据可以很容易的被分开。


近期也看了一篇Pacific Visualization 2016里利用LDA进行高维子空间的维度重建相关工作的论文【3】。


## 非线性降维方法
### MDS
MDS中文名叫多维尺度分析，主要考虑的是成对样本间的相似性，主要思想是用成对样本的相似性来构建合适的低维空间，使得样本在低维空间的距离和高位空间中的距离尽可能的保持一致。非常合理但是有很对的思想。根据样本是否可计量，可分为Metric MDS和Nonmetric MDS。目标函数是每对低维空间中的欧式距离与高维空间中相似度差的平方和，最小化目标函数，用一些数值优化方法来得到最优解。


## Isomap
Isomap的理论框架是MDS，但是高维空间中的距离变了，不再是欧式距离了，而是用测地线距离代替了欧式距离。所谓的测地线，就是流形上加速度为零的曲线，等同于欧式空间中的直线。如下图所示，s曲面上蓝色和红色的两点，欧式距离很小，而实际上他们之间没有路，因此必须沿着曲面走完测地线，所以他们之间的距离很大。


### SNE & t-SNE
这两种方法是Hinton组【4】提出的，可以说是state-of-art的方法。SNE想法与MDS类似，也是想将高维空间中的邻居信息保存到低维空间中，不过SNE的心比较小，它不是将所有点对直接的距离都保存下来，它只想将跟附近的邻居在一起，其他的离他本来就很远的点，根本不care。他的做法是将高维空间中点与点之间的欧式距离转化成条件概率，来代表相似度。低维空间中，也同样计算出这样一个概率。现在有了两个概率分布，怎么衡量他们之间的相似度呢？用KL散度！

KL散度是不平衡的，KL(p||q)与KL(q||p)不相对，这意味着不同种类的点对之间的距离错误在低维空间中没有得到同等对待。因此，symmetric SNE被提出来了。

t-SNE对于SNE改进是将高维空间中的欧氏距离换成了高斯分布，低维空间中，使用了heavy-tailed student t-distribution。带来了两个优点：

t-SNE的梯度强烈抵制不相似的点对在低维空间中被建模成距离很近。
尽管引入了不相似点对之间强烈的排斥，排斥力不会走向无穷。
t-SNE主要包括两个步骤：第一、t-SNE构建一个高维对象之间的概率分布，使得相似的对象有更高的概率被选择，而不相似的对象有较低的概率被选择。第二，t-SNE在低维空间里在构建这些点的概率分布，使得这两个概率分布之间尽可能的相似，同样也是来度量两个分布之间的相似性。

t-SNE可以将MNIST手写数字集几乎完全分开。在线尝试：http://scienceai.github.io/tsne-js/


t-SNE哪里都好，但是有一个致命的弱点，就是慢。不过，近期刚结束的WWW 2016上，华人学者唐健提出的LargeViz【5】，速度比t-SNE快了近30倍，想法很好！这种好的工作，比改改深度学习中网络的结构，调调参数，水一百篇论文更有意义！

### 参考文献
【1】Nan Cao, Yu-Ru Lin, and David Gotz
，UnTangle Map: Visual Analysis of Probabilistic Multi-Label Data ，IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS, VOL. 22, NO. 2, FEBRUARY 2016

【2】Stef van den Elzen, Danny Holten, Jorik Blaas, Jarke J. van Wijk, Reducing Snapshots to Points: A Visual Analytics Approach to Dynamic Network Exploration , IEEE TVCG(VAST’ 15) VOL. 22 NO.1 2016

【3】Fangfang Zhou ，Juncai Li ，Wei Huang ， Ying Zhao ， Xiaoru Yuan ， Xing Liang ，Yang Shi，Dimension Reconstruction for Visual Exploration of Subspace Clusters in High-dimensional Data ， IEEE Pacific Visualization 2016

【4】Laurens van der Maaten and Geoffrey Hinton，Visualizing data using t-SNE, Laurens van der Maaten and Geoffrey Hintton, Journel of machine learning research, 2008.

【5】Jian Tang, Jingzhou Liu, Ming Zhang and Qiaozhu Mei , Visualizing Large-scale and High-dimensional Data , WWW 2016